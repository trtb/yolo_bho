{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2178ba04",
   "metadata": {},
   "source": [
    "# CrowdHuman\n",
    "\n",
    "In this notebook, we prepare CrowdHuman data. Preparation essentially consists of transforming the annotations into the format supported by YoloV7 and creating the summary.txt referencing all the images in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e2336",
   "metadata": {},
   "source": [
    "## Links\n",
    "\n",
    "- https://www.crowdhuman.org/\n",
    "\n",
    "- Pre-treatment already done here: https://github.com/alaksana96/darknet-crowdhuman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8c945",
   "metadata": {},
   "source": [
    "## First step: Download\n",
    "\n",
    "In `homemade/`:\n",
    "\n",
    "```bash\n",
    "mkdir crowdhuman\n",
    "cd crowdhuman\n",
    "```\n",
    "\n",
    "Download all file from https://www.crowdhuman.org/\n",
    "<p>CrowdHuman_train01.zip\n",
    "    <a href=\"https://pan.baidu.com/s/1e-61WDiCqQibBVTIWqrssQ\">[Baidu Drive]</a> \n",
    "    <a href=\"https://drive.google.com/file/d/134QOvaatwKdy0iIeNqA_p-xkAhkV4F8Y/view\">[Google Drive]</a>\n",
    "</p>\n",
    "<p>CrowdHuman_train02.zip\n",
    "    <a href=\"https://pan.baidu.com/s/1OnndpWXiZxsCB3VtWEYE3w\">[Baidu Drive]</a>\n",
    "    <a href=\"https://drive.google.com/file/d/17evzPh7gc1JBNvnW1ENXLy5Kr4Q_Nnla/view\">[Google Drive]</a>\n",
    "</p>\n",
    "<p>CrowdHuman_train03.zip \n",
    "    <a href=\"https://pan.baidu.com/s/1kkfOlHV_xXKNbJUlLSkyXA\">[Baidu Drive]</a>\n",
    "    <a href=\"https://drive.google.com/file/d/1tdp0UCgxrqy1B6p8LkR-Iy0aIJ8l4fJW/view\">[Google Drive]</a>\n",
    "</p>\n",
    "<p>CrowdHuman_val.zip\n",
    "    <a href=\"https://pan.baidu.com/s/1kVBchjxOWu9sM5h8OAxfQw\">[Baidu Drive]</a>\n",
    "    <a href=\"https://drive.google.com/file/d/18jFI789CoHTppQ7vmRSFEdnGaSQZ4YzO/view\">[Google Drive]</a>\n",
    "</p>\n",
    "<p>annotation_train.odgt\n",
    "    <a href=\"https://pan.baidu.com/s/1wShABN_jYEiTRPM6_9-Cxg\">[Baidu Dirve]</a>\n",
    "    <a href=\"https://drive.google.com/file/d/1UUTea5mYqvlUObsC1Z8CFldHJAtLtMX3/view\">[Google Drive]</a>\n",
    "</p>\n",
    "<p>annotation_val.odgt\n",
    "    <a href=\"https://pan.baidu.com/s/1eObuAFcZyUw6PmUtpGS9vw\">[Baidu Drive]</a>\n",
    "    <a href=\"https://drive.google.com/file/d/10WIRwu8ju8GRLuCkZ_vT6hnNxs5ptwoL/view\">[Google Drive]</a>\n",
    "</p>\n",
    "<p>CrowdHuman_test.zip<br>\n",
    "    <a href=\"https://pan.baidu.com/s/133YKdndDTl9AWBRiVJJVRA\">[Baidu Drive]</a> Fetch Code: cr7k<br>\n",
    "    <a href=\"https://drive.google.com/file/d/1tQG3E_RrRI4wIGskorLTmDiWHH2okVvk/view\">[Google Drive]</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243db3c7",
   "metadata": {},
   "source": [
    "## Second step: Prepare repository\n",
    "\n",
    "- Train\n",
    "\n",
    "```bash\n",
    "unzip CrowdHuman_train01.zip\n",
    "unzip CrowdHuman_train02.zip\n",
    "unzip CrowdHuman_train03.zip\n",
    "mkdir images/\n",
    "mkdir images/train/\n",
    "mv Images/* images/train/\n",
    "```\n",
    "\n",
    "- Val\n",
    "\n",
    "```bash\n",
    "unzip CrowdHuman_val.zip\n",
    "mkdir images/val/\n",
    "mv Images/* images/val/\n",
    "```\n",
    "\n",
    "- Test\n",
    "\n",
    "```bash\n",
    "unzip CrowdHuman_test.zip\n",
    "mkdir images/test/\n",
    "mv Images/* images/test/\n",
    "```\n",
    "\n",
    "- Clean\n",
    "\n",
    "```bash\n",
    "rmdir Images/\n",
    "rm unzip CrowdHuman_train01.zip\n",
    "rm unzip CrowdHuman_train02.zip\n",
    "rm unzip CrowdHuman_train03.zip\n",
    "rm CrowdHuman_val.zip\n",
    "rm CrowdHuman_test.zip\n",
    "```\n",
    "\n",
    "Normally, you should now have this tree structure:\n",
    "\n",
    "```bash\n",
    "$ tree -L 2\n",
    ".\n",
    "├── annotation_train.odgt\n",
    "├── annotation_val.odgt\n",
    "└── images\n",
    "    ├── test\n",
    "    ├── train\n",
    "    └── val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822e62b",
   "metadata": {},
   "source": [
    "## Third step: Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54955597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"homemade/crowdhuman\")\n",
    "repositories = ['train', 'val']\n",
    "path_images = path / 'images'\n",
    "odgt_format = path / \"annotation_{}.odgt\"\n",
    "\n",
    "path_labels = path / 'labels'\n",
    "path_labels.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795aa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_annotations(line, images, labels):\n",
    "    dict_line = json.loads(line)\n",
    "    \n",
    "    image_id = dict_line['ID']\n",
    "    image_file = images / (image_id + '.jpg')\n",
    "    \n",
    "    img = Image.open(image_file)\n",
    "    width, height = img.size\n",
    "    \n",
    "    strings = []\n",
    "    for label in dict_line['gtboxes']:\n",
    "        if 'extra' in label:\n",
    "            if label['extra'].get('ignore', 0) == 1 or label['extra'].get('unsure', 0) == 1:\n",
    "                continue\n",
    "            \n",
    "        bb = label['hbox'] # x, y, width, height\n",
    "        \n",
    "        bb = [min(max(bb[0], 0), width), \n",
    "              min(max(bb[1], 0), height),\n",
    "              min(max(bb[0] + bb[2], 0), width), \n",
    "              min(max(bb[1] + bb[3], 0), height)] # xmin, ymin, xmax, ymax\n",
    "        \n",
    "        x_center = (bb[0] + bb[2]) / 2\n",
    "        x_size = (bb[2] - bb[0])\n",
    "        y_center = (bb[1] + bb[3]) / 2\n",
    "        y_size = (bb[3] - bb[1])\n",
    "        \n",
    "        if x_size <= 3 or y_size <= 3:\n",
    "            continue\n",
    "            \n",
    "        x_center /= width\n",
    "        x_size /= width\n",
    "        y_center /= height\n",
    "        y_size /= height\n",
    "        \n",
    "        strings.append(\"{} {:.6f} {:.6f} {:.6f} {:.6f}\".format(0, x_center, y_center, x_size, y_size))\n",
    "        \n",
    "    if len(strings) > 0:\n",
    "        output_file = labels / (image_id + '.txt')\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"\\n\".join(strings) + \"\\n\")\n",
    "        return True, str(image_file)\n",
    "    return False, str(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03ea458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing homemade/crowdhuman/annotation_train.odgt:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:05<00:00, 2918.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid images: homemade/crowdhuman/images/train/282555,1e5f7000c479116e.jpg\n",
      "Processing homemade/crowdhuman/annotation_val.odgt:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4370/4370 [00:01<00:00, 2896.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid images: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for rep in repositories:\n",
    "    odgt_file = str(odgt_format).format(rep)\n",
    "    print(\"Processing {}:\".format(odgt_file))\n",
    "\n",
    "    with open(odgt_file) as f:\n",
    "        image_list = f.read().split('\\n')\n",
    "    image_list = list(filter(len, image_list))\n",
    "\n",
    "    valid_images = []\n",
    "    invalid_images = []\n",
    "    \n",
    "    images = path_images / rep\n",
    "    labels = path_labels / rep\n",
    "    \n",
    "    labels.mkdir(exist_ok=True)\n",
    "\n",
    "    for line in tqdm(image_list):\n",
    "        valid, image_file = generate_annotations(line, images, labels)\n",
    "        if valid:\n",
    "            valid_images.append(image_file)\n",
    "        else:\n",
    "            invalid_images.append(image_file)\n",
    "\n",
    "    with open(labels / \"summary.txt\", 'w') as f:\n",
    "        f.write(\"\\n\".join(valid_images) + \"\\n\")\n",
    "\n",
    "    print(\"Invalid images: {}\".format(\"\\n\".join(invalid_images)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
